# EVOLVE-BLOCK-START

from math import gcd
from functools import lru_cache

# -------------- Geometry and utilities --------------

def overlaps(a, b):
    """Open-interval overlap test: True iff intervals overlap."""
    (l1, r1), (l2, r2) = a, b
    return max(l1, l2) < min(r1, r2)

def firstfit_colors(intervals):
    """
    Simulate FirstFit on open intervals presented in order.
    Return number of colors used.
    """
    colors = []  # list of color classes; each is a list of intervals
    for iv in intervals:
        placed = False
        for cls in colors:
            # robust: check against all in color
            conflict = False
            for u in cls:
                if overlaps(u, iv):
                    conflict = True
                    break
            if not conflict:
                cls.append(iv)
                placed = True
                break
        if not placed:
            colors.append([iv])
    return len(colors)

def clique_number(intervals):
    """
    Compute omega = max number of open intervals covering a point,
    handling right endpoints before left endpoints at ties.
    """
    events = []
    for (l, r) in intervals:
        if l >= r:
            continue
        events.append((l, +1))
        events.append((r, -1))
    events.sort(key=lambda e: (e[0], 0 if e[1] == -1 else 1))
    cur = best = 0
    for _, t in events:
        cur += t
        if cur > best:
            best = cur
    return best

def normalize_to_grid(intervals):
    """
    Normalize endpoints to a compact integer grid:
      - Collect all unique endpoints,
      - Map to consecutive even integers (step 2),
      - Return integer intervals in the same arrival order.
    """
    if not intervals:
        return []
    endpoints = sorted(set(x for seg in intervals for x in seg))
    coord = {}
    cur = 0
    for e in endpoints:
        coord[e] = cur
        cur += 2
    return [(coord[l], coord[r]) for (l, r) in intervals]

def gcd_shrink(intervals):
    """
    Optionally shrink integer coordinates by dividing by global gcd.
    Keep order and lengths positive.
    """
    if not intervals:
        return intervals
    vals = []
    for (l, r) in intervals:
        vals.append(abs(l))
        vals.append(abs(r))
    g = 0
    for v in vals:
        g = gcd(g, v)
    if g > 1:
        return [(l // g, r // g) for (l, r) in intervals]
    return intervals

# -------------- Backbone builder (with micro-perturbations) --------------

def build_backbone(base_seed, depth, offsets, blockers, extra_first=False, micro_shift_idx=None, epsilon=0.0):
    """
    Build recursive 4-copy + 4-blocker backbone with optional micro-perturbation.
    Parameters:
      - base_seed: list of (l, r) floats
      - depth: int
      - offsets: tuple of 4 integers (canonical copy starts)
      - blockers: list of 4 tuples (a, b) for long connectors (scaled by delta)
      - extra_first: if True, add one extra copy offset = offsets[-1] + 4
      - micro_shift_idx: index in {0..3} of the copy to perturb per level (None to disable)
      - epsilon: tiny fractional shift multiplier of delta applied to chosen copy
    """
    T = [tuple(iv) for iv in base_seed]
    for lvl in range(depth):
        lo = min(l for l, r in T)
        hi = max(r for l, r in T)
        delta = hi - lo
        S = []
        offs = list(offsets)
        if extra_first:
            offs = offs + [offs[-1] + 4]
        for i, start in enumerate(offs):
            # compute offset; if i equals micro_shift_idx (and within first 4), apply micro shift
            mshift = 0.0
            if micro_shift_idx is not None and i < 4 and i == micro_shift_idx and epsilon != 0.0:
                mshift = epsilon * delta
            off = delta * start - lo + mshift
            for (l, r) in T:
                S.append((l + off, r + off))
        # add blockers (not shifted)
        for (a, b) in blockers:
            S.append((delta * a, delta * b))
        T = S
    return T

# -------------- Deterministic wave augmentation --------------

def add_waves_deterministic(intervals, max_waves=48, wave_lengths=(2, 3)):
    """
    Deterministically add short 'wave' intervals aligned to inter-copy gaps.
    Only accept additions that strictly increase FirstFit colors and keep omega unchanged.
    """
    T = normalize_to_grid(intervals)
    if not T:
        return T
    base_cols = firstfit_colors(T)
    base_om = clique_number(T)
    if base_om == 0:
        return T

    coords = sorted(set(x for iv in T for x in iv))
    # candidate anchors near gaps: left neighbor + 1, mid, right - wave_len - 1
    anchors = []
    for a, b in zip(coords, coords[1:]):
        if b - a >= 3:
            anchors.append(a + 1)
            anchors.append((a + b) // 2)
            anchors.append(b - 2)

    # Alternate wave types and positions deterministically
    waves_added = 0
    idx = 0
    while waves_added < max_waves and idx < len(anchors):
        wl = wave_lengths[waves_added % len(wave_lengths)]
        left = anchors[idx]
        idx += 1
        cand = (left, left + wl)
        if cand[0] >= cand[1]:
            continue
        new_om = clique_number(T + [cand])
        if new_om > base_om:
            continue
        new_cols = firstfit_colors(T + [cand])
        if new_cols > base_cols:
            T = normalize_to_grid(T + [cand])
            base_cols = new_cols
            # base_om unchanged by design guard
            waves_added += 1
    return T

# -------------- Frontier-preserving pruning --------------

def frontier_prune(intervals, keep_ratio):
    """
    Two-stage deterministic pruning:
      Stage 1: remove intervals in decreasing length order if ratio is preserved.
      Stage 2: second pass in original arrival order to shave redundant small intervals.
    """
    cur = list(intervals)
    if not cur:
        return cur

    def observe(A):
        if not A:
            return (0, 0, 0.0)
        Tn = normalize_to_grid(A)
        cols = firstfit_colors(Tn)
        om = clique_number(Tn) or 1
        return (cols, om, cols / om)

    base_cols, base_om, base_ratio = observe(cur)
    target = max(base_ratio, keep_ratio)

    # Stage 1: remove by decreasing length
    def length(iv): return iv[1] - iv[0]
    changed = True
    while changed:
        changed = False
        order = sorted(range(len(cur)), key=lambda i: (-length(cur[i]), i))
        for idx in order:
            cand = cur[:idx] + cur[idx+1:]
            cols, om, ratio = observe(cand)
            if om > 0 and ratio >= target - 1e-12:
                cur = cand
                changed = True
                break

    # Stage 2: pass in arrival order
    i = 0
    while i < len(cur):
        cand = cur[:i] + cur[i+1:]
        cols, om, ratio = observe(cand)
        if om > 0 and ratio >= target - 1e-12:
            cur = cand
            # don't increment i, recheck same index
        else:
            i += 1
    return cur

# -------------- Pattern-aware memoization --------------

_eval_cache = {}

def eval_signature(depth, offsets_id, blockers_id, extra_first, micro_shift_idx, epsilon, base_seed_id):
    return (depth, offsets_id, blockers_id, int(extra_first), micro_shift_idx if micro_shift_idx is not None else -1, epsilon, base_seed_id)

def evaluate_instance(intervals):
    Tn = normalize_to_grid(intervals)
    cols = firstfit_colors(Tn)
    om = clique_number(Tn) or 1
    return cols, om, cols / om, Tn

# -------------- High-level constructor --------------

def construct_intervals():
    """
    Build a sequence of open intervals to maximize FirstFit colors while keeping clique small.
    Strategy:
      - Cycle over canonical backbones (A/B/C/D) and blocker templates,
      - Try depths {3,4,5} and micro-perturbations,
      - Optionally add deterministic waves,
      - Frontier-preserving prune,
      - Return best normalized instance by FF/Ï‰, then by size, then by colors.
    """
    base_seeds = [
        (0, [(0.0, 1.0)]),            # id 0
        (1, [(0.0, 1.0), (2.0, 3.0)]),# id 1
    ]
    # Four canonical offset patterns (backbone cycle)
    offset_cycle = [
        (0, (2, 6, 10, 14)),  # A
        (1, (1, 5, 9, 13)),   # B
        (2, (3, 7, 11, 15)),  # C
        (3, (0, 4, 8, 12)),   # D
    ]
    blocker_templates = [
        (0, ((1,5),(12,16),(4,9),(8,13))),   # canonical
        (1, ((0,4),(11,15),(3,8),(7,12))),   # shifted
        (2, ((2,6),(12,16),(4,9),(8,13))),   # variant
    ]
    depths = [3, 4, 5]
    extra_first_choices = [False, True]
    micro_shift_indices = [None, 0, 1, 2, 3]
    epsilons = [0.0, 0.25, 0.5]  # fraction of delta

    best = None  # (ratio, -n, cols, om, normalized_intervals)

    for base_id, base in base_seeds:
        for depth in depths:
            for offsets_id, offs in offset_cycle:
                for blockers_id, blks in blocker_templates:
                    for extra in extra_first_choices:
                        for ms_idx in micro_shift_indices:
                            for eps in epsilons:
                                sig = eval_signature(depth, offsets_id, blockers_id, extra, ms_idx, eps, base_id)
                                if sig in _eval_cache:
                                    cols, om, ratio, Tn = _eval_cache[sig]
                                else:
                                    T = build_backbone(
                                        base_seed=base,
                                        depth=depth,
                                        offsets=offs,
                                        blockers=blks,
                                        extra_first=extra,
                                        micro_shift_idx=ms_idx,
                                        epsilon=eps
                                    )
                                    cols, om, ratio, Tn = evaluate_instance(T)
                                    _eval_cache[sig] = (cols, om, ratio, Tn)

                                # Accept only reasonable clique (prevent blow-ups)
                                if om == 0:
                                    continue

                                # Optional deterministic waves (attempt once per candidate)
                                Tw = add_waves_deterministic(Tn, max_waves=24, wave_lengths=(2, 3))
                                cols_w = firstfit_colors(Tw)
                                om_w = clique_number(Tw) or 1
                                ratio_w = cols_w / om_w

                                # Pick better of plain vs wave-augmented
                                if ratio_w > ratio + 1e-12 or (abs(ratio_w - ratio) <= 1e-12 and len(Tw) < len(Tn)):
                                    candT = Tw
                                    cand_cols, cand_om, cand_ratio = cols_w, om_w, ratio_w
                                else:
                                    candT = Tn
                                    cand_cols, cand_om, cand_ratio = cols, om, ratio

                                # Frontier-preserving pruning
                                pruned = frontier_prune(candT, cand_ratio)
                                pruned = normalize_to_grid(pruned)
                                pruned = gcd_shrink(pruned)
                                p_cols = firstfit_colors(pruned)
                                p_om = clique_number(pruned) or 1
                                p_ratio = p_cols / p_om

                                # Safety: if pruning hurt (shouldn't), revert
                                if p_ratio + 1e-12 < cand_ratio:
                                    finalT = candT
                                    f_cols, f_om, f_ratio = cand_cols, cand_om, cand_ratio
                                else:
                                    finalT = pruned
                                    f_cols, f_om, f_ratio = p_cols, p_om, p_ratio

                                n = len(finalT)
                                item = (f_ratio, -n, f_cols, f_om, finalT)
                                if (best is None or
                                    item[0] > best[0] + 1e-12 or
                                    (abs(item[0] - best[0]) <= 1e-12 and (item[1] > best[1] or (item[1] == best[1] and item[2] > best[2])))):
                                    best = item

    # Fallback if search fails (unlikely): canonical depth=4
    if best is None:
        T = build_backbone([(0.0, 1.0)], depth=4, offsets=(2,6,10,14), blockers=((1,5),(12,16),(4,9),(8,13)))
        Tn = normalize_to_grid(T)
        Tn = gcd_shrink(Tn)
        return Tn

    return best[4]

# EVOLVE-BLOCK-END

def run_experiment(**kwargs):
    """Main called by evaluator"""
    return construct_intervals()